{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Style Transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjDOhOY87bIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Ev1yAOOwQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getModelWeightsAsDict():\n",
        "  \n",
        "  keras.backend.clear_session()\n",
        "  tf.reset_default_graph()\n",
        "  model = keras.applications.VGG19(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),include_top=False,weights='imagenet')\n",
        "  \n",
        "  with keras.backend.get_session() as sess:\n",
        "    weights_dict = {}\n",
        "    for i in range(len(model.layers)):\n",
        "      weights_dict['layer_' + str(i)] = []\n",
        "      for j in range(len(model.layers[i].weights)):\n",
        "        weights_dict['layer_' + str(i)].append(sess.run(model.layers[i].weights[j]))\n",
        "  tf.reset_default_graph()\n",
        "  return weights_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uendrXVjVc3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_nst_model(weights_dict):\n",
        "  \n",
        "  layers = {}\n",
        "\n",
        "  image_shape = (IMAGE_SIZE,IMAGE_SIZE,3)\n",
        "  layers['input'] = tf.Variable(initial_value=tf.initializers.random_normal().__call__((1,)+image_shape),expected_shape=(1,)+image_shape,\n",
        "                       name='nst_output',dtype=tf.float32)\n",
        "\n",
        "  layers['conv1_1'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['input'],weights_dict['layer_1'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_1'][1]))\n",
        "  layers['conv1_2'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv1_1'],weights_dict['layer_2'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_2'][1]))\n",
        "  layers['pool1'] = tf.nn.avg_pool(layers['conv1_2'],ksize=(1,2,2,1),strides=(1,2,2,1),padding='VALID')\n",
        "\n",
        "  layers['conv2_1'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['pool1'],weights_dict['layer_4'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_4'][1]))\n",
        "  layers['conv2_2'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv2_1'],weights_dict['layer_5'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_5'][1]))\n",
        "  layers['pool2'] = tf.nn.avg_pool(layers['conv2_2'],ksize=(1,2,2,1),strides=(1,2,2,1),padding='VALID')\n",
        "\n",
        "\n",
        "  layers['conv3_1'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['pool2'],weights_dict['layer_7'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_7'][1]))\n",
        "  layers['conv3_2'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv3_1'],weights_dict['layer_8'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_8'][1]))\n",
        "  layers['conv3_3'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv3_2'],weights_dict['layer_9'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_9'][1]))\n",
        "  layers['conv3_4'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv3_3'],weights_dict['layer_10'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_10'][1]))\n",
        "\n",
        "  layers['pool3'] = tf.nn.avg_pool(layers['conv3_4'],ksize=(1,2,2,1),strides=(1,2,2,1),padding='VALID')\n",
        "\n",
        "\n",
        "  layers['conv4_1'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['pool3'],weights_dict['layer_12'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_12'][1]))\n",
        "  layers['conv4_2'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv4_1'],weights_dict['layer_13'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_13'][1]))\n",
        "  layers['conv4_3'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv4_2'],weights_dict['layer_14'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_14'][1]))\n",
        "  layers['conv4_4'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv4_3'],weights_dict['layer_15'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_15'][1]))\n",
        "\n",
        "  layers['pool4'] = tf.nn.avg_pool(layers['conv4_4'],ksize=(1,2,2,1),strides=(1,2,2,1),padding='VALID')\n",
        "\n",
        "\n",
        "\n",
        "  layers['conv5_1'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['pool4'],weights_dict['layer_17'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_17'][1]))\n",
        "  layers['conv5_2'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv5_1'],weights_dict['layer_18'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_18'][1]))\n",
        "  layers['conv5_3'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv5_2'],weights_dict['layer_19'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_19'][1]))\n",
        "  layers['conv5_4'] = tf.nn.relu(tf.nn.bias_add(tf.nn.convolution(layers['conv5_3'],weights_dict['layer_20'][0],padding='SAME',strides=(1,1)),\n",
        "                                            weights_dict['layer_20'][1]))\n",
        "\n",
        "  layers['pool5'] = tf.nn.avg_pool(layers['conv5_4'],ksize=(1,2,2,1),strides=(1,2,2,1),padding='VALID')\n",
        "  \n",
        "  return layers\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI-gwzCJpuhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTENT_LAYERS = [\n",
        "#     ('conv1_2', 0.2),\n",
        "#     ('conv2_2', 0.2),\n",
        "#     ('conv3_2', 0.2),\n",
        "    ('conv4_2', 1.0),\n",
        "#     ('conv5_2', 1.0),\n",
        "]\n",
        "STYLE_LAYERS = [\n",
        "    ('conv1_1', 0.2),\n",
        "    ('conv2_1', 0.2),\n",
        "    ('conv3_1', 0.2),\n",
        "    ('conv4_1', 0.2),\n",
        "    ('conv5_1', 0.2)\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "def getContentLoss(layers,content_target,content_layer_name):\n",
        "  \n",
        "  m,H,W,C = content_target.shape\n",
        "  return tf.reduce_sum(tf.pow(content_target-layers[content_layer_name],2))/(4.0*C*H*W)\n",
        "\n",
        "\n",
        "def getStyleLoss(layers,style_target,style_layer_name):\n",
        "  m,H,W,C = style_target.shape\n",
        "  \n",
        "  style_target = tf.transpose(tf.reshape(style_target,[H*W,C]))\n",
        "  gram_target = tf.matmul(style_target,tf.transpose(style_target))\n",
        "  \n",
        "  style_tensor = tf.transpose(tf.reshape(layers[style_layer_name],[H*W,C]))\n",
        "  gram_tensor = tf.matmul(style_tensor,tf.transpose(style_tensor))  \n",
        "  \n",
        "  return tf.reduce_sum(tf.pow(gram_tensor-gram_target,2))/(4.0*C*C*H*H*W*W)\n",
        "\n",
        "def getTotalLoss(J_content,J_style,alpha,beta):\n",
        "  return J_content*alpha + J_style*beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB3Ag0VZZDMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MEANS = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3)) \n",
        "    \n",
        "  \n",
        "def load_image(filename):\n",
        "  \n",
        "  image = cv2.imread(filename)\n",
        "  image = cv2.resize(image,(IMAGE_SIZE,IMAGE_SIZE))\n",
        "  cv2_imshow(image)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  image = image.reshape((1,)+image.shape)\n",
        "  \n",
        "  image = image - MEANS\n",
        "  return image\n",
        "\n",
        "\n",
        "def save_image(image,filename):\n",
        "  #image = (image + 1) * 127.5\n",
        "  image = image + MEANS\n",
        "  image = np.clip(image[0], 0, 255).astype('uint8')\n",
        "  scipy.misc.imsave(filename, image)\n",
        "  return image\n",
        "  \n",
        "\n",
        "def generate_noise_image():\n",
        "    # Generate a random noise_image\n",
        "    noise_image = np.random.uniform(0, 255, (1, IMAGE_SIZE, IMAGE_SIZE, 3)).astype('uint8')\n",
        "    noise_image = noise_image - MEANS\n",
        "    return noise_image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neGr4-m5noDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_style_transfer(content_image_filename,\n",
        "                      style_image_filename,\n",
        "                       initialImage=None,\n",
        "                      alpha=10,beta=1e-1,\n",
        "                      epochs=1200,\n",
        "                       learning_rate=5.0,\n",
        "                      prefix=None):\n",
        "  \n",
        "  if prefix is None:\n",
        "    print('Output file prefix not defined...')\n",
        "    return\n",
        "  \n",
        "  content_image = load_image(content_image_filename)\n",
        "  style_image = load_image(style_image_filename)\n",
        "  \n",
        "  \n",
        "\n",
        "  weights_dict = getModelWeightsAsDict()\n",
        "  tf.reset_default_graph()\n",
        "  layers = get_nst_model(weights_dict)\n",
        "\n",
        "  print('Model graph generated...\\n')\n",
        "  \n",
        "  # Calculate tensor for content loss\n",
        "  J_content = 0.0\n",
        "  with tf.Session() as sess:\n",
        "    for content_layer_name, weight in CONTENT_LAYERS:\n",
        "      content_layer = layers[content_layer_name]\n",
        "      content_target = sess.run(content_layer,feed_dict={layers['input']:content_image})\n",
        "      J_content = J_content + weight * getContentLoss(layers,content_target, content_layer_name)\n",
        "  print('Content loss defined...\\n')\n",
        "  \n",
        "  \n",
        "  # Calculate tensor for style loss\n",
        "  J_style = 0.0\n",
        "  with tf.Session() as sess:\n",
        "    for style_layer_name, weight in STYLE_LAYERS:\n",
        "        style_layer = layers[style_layer_name]\n",
        "        style_target = sess.run(style_layer,feed_dict={layers['input']:style_image})\n",
        "        J_style = J_style + weight * getStyleLoss(layers,style_target,style_layer_name)\n",
        "  print('Style loss defined...\\n')\n",
        "  \n",
        "  J_total = getTotalLoss(J_content,J_style,alpha=100,beta=1e-1)\n",
        "\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate).minimize(J_total)\n",
        "\n",
        "  print('Losses defined...\\n')\n",
        "  \n",
        "  print('Trainable variables:\\n',tf.trainable_variables())\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    \n",
        "    # Initialize image\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    if initialImage is not None:\n",
        "      sess.run(layers['input'].assign(initialImage))\n",
        "    else:\n",
        "      sess.run(layers['input'].assign(generate_noise_image()))\n",
        "      \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      epoch_loss, epoch_content_loss, epoch_style_loss, _ = sess.run([J_total,J_content,J_style,optimizer])\n",
        "\n",
        "      if (epoch+1) % 250 == 0:\n",
        "        generated_image = sess.run(layers['input'])\n",
        "        generated_image = save_image(generated_image,prefix + '_' + str(epoch+1) + '.jpg')\n",
        "        print('Loss after epoch %d: \\nT: %f, \\nC: %f, \\nS: %f'%(epoch,epoch_loss,epoch_content_loss,epoch_style_loss))\n",
        "        generated_image = cv2.cvtColor(generated_image, cv2.COLOR_RGB2BGR)\n",
        "        cv2_imshow(generated_image)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--D6DTBWHwd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example Usage\n",
        "IMAGE_SIZE = 320\n",
        "content_image_filename = './dog.jpg'\n",
        "style_image_filename = './/starry_night.jpg'\n",
        "run_style_transfer(content_image_filename=content_image_filename,\n",
        "                  style_image_filename=style_image_filename,\n",
        "                  epochs=1000,\n",
        "                   learning_rate=10.0,\n",
        "                  prefix='dog-starry_night')\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}